{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":276428,"sourceType":"datasetVersion","datasetId":115588}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ahmedwael206/bigram-statistical-model-to-neural-network-model?scriptVersionId=254464800\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Step 1: Imports and Logging\nimport torch\nimport torch.nn.functional as F\nimport string\nimport os\n\nprint(\"[INFO] Starting Bigram Model Preparation\")\n\n# Dataset file path (update this path after uploading to Kaggle input)\ndataset_path = \"/kaggle/input/shakespeareonline/t8.shakespeare.txt\"\nassert os.path.exists(dataset_path), \"[ERROR] Dataset not found!\"\n\nprint(f\"[INFO] Dataset path found: {dataset_path}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:12:42.445564Z","iopub.execute_input":"2025-08-01T13:12:42.445967Z","iopub.status.idle":"2025-08-01T13:12:42.455227Z","shell.execute_reply.started":"2025-08-01T13:12:42.445923Z","shell.execute_reply":"2025-08-01T13:12:42.454067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Read and preprocess dataset\nwith open(dataset_path, 'r', encoding='utf-8') as f:\n    raw_text = f.read()\n\nprint(f\"[INFO] Loaded {len(raw_text)} raw characters\")\n\n# Lowercase and tokenize by words\nimport re\nwords = re.findall(r'\\b[a-zA-Z]+\\b', raw_text.lower())  # extract words only\nprint(f\"[INFO] Found {len(words)} words\")\n\n# Add start/end token '.' around each word and join them with newlines\ntokenized_words = ['.' + word + '.' for word in words]\ntext = '\\n'.join(tokenized_words)\n\nprint(f\"[INFO] Tokenized and marked words (first 5 lines):\")\nprint(\"\\n\".join(tokenized_words[:5]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:22:14.900498Z","iopub.execute_input":"2025-08-01T13:22:14.900856Z","iopub.status.idle":"2025-08-01T13:22:15.542417Z","shell.execute_reply.started":"2025-08-01T13:22:14.900824Z","shell.execute_reply":"2025-08-01T13:22:15.541403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Updated vocabulary including '.'\nvocab = list(string.ascii_lowercase) + ['.']\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\n\nprint(f\"[INFO] Vocabulary size (including '.'): {len(vocab)}\")\nprint(f\"[DEBUG] stoi: {stoi}\")\n\n# Step 4: Initialize Count Matrix\ncount = torch.zeros((27, 27), dtype=torch.int32)\nprint(\"[INFO] Initialized 27x27 count matrix\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:25:50.304612Z","iopub.execute_input":"2025-08-01T13:25:50.304903Z","iopub.status.idle":"2025-08-01T13:25:50.311877Z","shell.execute_reply.started":"2025-08-01T13:25:50.304883Z","shell.execute_reply":"2025-08-01T13:25:50.310742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Fill count matrix line-by-line (word-by-word)\nfor line in text.splitlines():\n    for ch1, ch2 in zip(line, line[1:]):\n        i = stoi[ch1]\n        j = stoi[ch2]\n        count[i, j] += 1\n\nprint(f\"[INFO] Filled bigram counts for all tokenized words.\")\nprint(f\"[DEBUG] Count matrix sample:\\n{count[:5, :5]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:25:52.389384Z","iopub.execute_input":"2025-08-01T13:25:52.389703Z","iopub.status.idle":"2025-08-01T13:27:18.312099Z","shell.execute_reply.started":"2025-08-01T13:25:52.389683Z","shell.execute_reply":"2025-08-01T13:27:18.311075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5.5: Full bigram count matrix heatmap with annotations\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(16, 14))\nplt.imshow(count, cmap='Blues')\nplt.colorbar(label='Count')\n\n# Set axis labels and ticks\nplt.xticks(ticks=range(len(vocab)), labels=vocab)\nplt.yticks(ticks=range(len(vocab)), labels=vocab)\nplt.xlabel('Next Character')\nplt.ylabel('Current Character')\nplt.title('Bigram Count Matrix (27x27) — Characters: a-z and .')\n\n# Annotate every non-zero count\nfor i in range(len(vocab)):\n    for j in range(len(vocab)):\n        val = count[i, j].item()\n        if val > 0:\n            fontsize = 6 if val < 100 else 8 if val < 1000 else 10\n            plt.text(j, i, str(val), ha='center', va='center', color='black', fontsize=fontsize)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:33:11.811313Z","iopub.execute_input":"2025-08-01T13:33:11.812397Z","iopub.status.idle":"2025-08-01T13:33:13.534323Z","shell.execute_reply.started":"2025-08-01T13:33:11.812354Z","shell.execute_reply":"2025-08-01T13:33:13.533203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Normalize rows to convert counts to probabilities\nprob = count.float()\nrow_sums = prob.sum(dim=1, keepdim=True)\nprob /= row_sums + 1e-8  # avoid divide-by-zero\n\nprint(f\"[INFO] Converted count matrix to probability matrix\")\nprint(f\"[DEBUG] Probability row for 'a': {prob[stoi['a']]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:05:04.235123Z","iopub.execute_input":"2025-08-01T14:05:04.235826Z","iopub.status.idle":"2025-08-01T14:05:04.296114Z","shell.execute_reply.started":"2025-08-01T14:05:04.23579Z","shell.execute_reply":"2025-08-01T14:05:04.294942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Sample one full word from the bigram model (until '.' is generated)\ndef sample_word(start_char='.'):\n    idx = stoi[start_char]\n    word = ''\n    \n    print(f\"[INFO] Starting generation from: '{start_char}'\")\n    \n    while True:\n        probs = prob[idx]\n        next_idx = torch.multinomial(probs, num_samples=1).item()\n        next_char = itos[next_idx]\n\n        print(f\"[DEBUG] '{itos[idx]}' -> '{next_char}'\")\n        \n        if next_char == '.':\n            break  # stop at end token\n        word += next_char\n        idx = next_idx  # continue from new char\n\n    return word\n\n# Generate a few sample words\nprint(\"[RESULT] Sampled words:\")\nfor _ in range(10):\n    print(sample_word())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:07:30.410336Z","iopub.execute_input":"2025-08-01T14:07:30.410755Z","iopub.status.idle":"2025-08-01T14:07:30.424522Z","shell.execute_reply.started":"2025-08-01T14:07:30.410729Z","shell.execute_reply":"2025-08-01T14:07:30.423408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📉 Limitations of Character-Level Bigram Models\n\nA **character-level Bigram model** predicts the next character based only on the **current character**:\n\n> **P(cₙ | cₙ₋₁)**\n\nThis means it only considers a **single character of history** when making predictions. While it's simple and fast, it leads to major limitations.\n\n---\n\n## ⚠️ Why Bigram Models Fail to Generalize\n\n### 🔹 1. **Lack of Context**\nThe model does not account for multi-character patterns or word structures. For example, it might know that:\n\n- `'q'` is followed by `'u'` (which is good),  \n- but it **doesn't remember** what came before `'q'`.\n\nSo it might generate:\n\ntqush\nglory\nblenquest","metadata":{}},{"cell_type":"markdown","source":"\n*These are examples of **hallucination** — outputs that seem plausible but aren’t real.*****\n\n---\n\n### 🔹 2. **One-Character Dependency**\n\nEvery character prediction depends **only** on the one before it. That means:\n\n- It ignores all characters before `cₙ₋₁`\n- It cannot enforce long-term consistency in a word\n- It cannot \"remember\" spelling rules like `\"tion\"` or `\"str\"`\n\n---\n\n### 🔹 3. **Frequency Dominance & Repetition**\n\nBecause the model is trained on raw frequency counts:\n\n```python\nP(j | i) = count[i][j] / sum(count[i])\n","metadata":{}},{"cell_type":"markdown","source":"# 🤖 Why Use a Neural Network Instead of Count-Based Bigram Model?\n\nThe classic count-based bigram model estimates:\n\n> **P(c₂ | c₁) = count[c₁][c₂] / sum(count[c₁])**\n\nIt simply memorizes how frequently each character follows another. However, this has several limitations:\n\n---\n\n## 🚫 Problems with Count-Based Bigram Models\n\n- **No Generalization**: If a bigram has never occurred, its probability is 0 — even if it's linguistically valid.\n- **Overfitting on Frequent Bigrams**: Common pairs like `'t' → 'h'` dominate, drowning out less common but valid patterns.\n- **No Learning Mechanism**: There's no optimization or adaptation based on mistakes — it's all just raw frequency.\n\n---\n\n## ✅ Benefits of a Neural Bigram Model\n\n| Feature                      | Count-Based | Neural Net |\n|------------------------------|-------------|-------------|\n| Learns from mistakes         | ❌          | ✅          |\n| Can generalize patterns      | ❌          | ✅          |\n| Differentiable and trainable | ❌          | ✅          |\n| Learns low-rank structure    | ❌          | ✅          |\n| Tunable via validation loss  | ❌          | ✅          |\n\n---\n\n## 🧪 Train/Test Split for Real Evaluation\n\nTo prevent overfitting and evaluate the model's generalization, we split our data:\n\n- **80% for training**\n- **20% for testing**\n\n> We expect a good neural model to **perform well on unseen character pairs**, while the count-based model cannot.\n\n---\n\n## 📌 Conclusion\n\nThe neural model doesn't just memorize — it **learns** to smooth probabilities, generalize to similar patterns, and adapt based on training data.\n\nThis makes it far more powerful — even with the same number of parameters as a count matrix (just a 27×27 weight matrix).\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport string\nimport random\n\n# Vocabulary includes a-z and the special '.'\nvocab = list(string.ascii_lowercase) + ['.']\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(vocab)\n\nprint(f\"[INFO] Vocabulary size: {vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T23:27:52.632769Z","iopub.execute_input":"2025-08-05T23:27:52.633033Z","iopub.status.idle":"2025-08-05T23:27:57.706803Z","shell.execute_reply.started":"2025-08-05T23:27:52.633014Z","shell.execute_reply":"2025-08-05T23:27:57.705545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize corpus: each word becomes `.word.`\ndataset_path = \"/kaggle/input/shakespeareonline/t8.shakespeare.txt\"\nwith open(dataset_path, 'r', encoding='utf-8') as f:\n    raw_text = f.read().lower()\nimport re\nwords = re.findall(r'\\b[a-zA-Z]+\\b', raw_text)\ntokens = ['.' + w + '.' for w in words]\n\n# Collect input/output pairs from all bigrams\nxs = []\nys = []\n\nfor token in tokens:\n    for ch1, ch2 in zip(token, token[1:]):\n        xs.append(stoi[ch1])\n        ys.append(stoi[ch2])\n\nx_tensor = torch.tensor(xs)  # input indices\ny_tensor = torch.tensor(ys)  # target indices\n\nprint(f\"[INFO] Prepared {len(xs)} (x, y) training pairs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T23:28:00.762086Z","iopub.execute_input":"2025-08-05T23:28:00.763702Z","iopub.status.idle":"2025-08-05T23:28:03.824061Z","shell.execute_reply.started":"2025-08-05T23:28:00.763669Z","shell.execute_reply":"2025-08-05T23:28:03.822471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step: Train/Test Split (80% train, 20% test)\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    x_tensor, y_tensor, test_size=0.2, random_state=42, shuffle=True\n)\n\nprint(f\"[INFO] Training samples: {x_train.shape[0]}\")\nprint(f\"[INFO] Test samples:     {x_test.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T23:28:10.368176Z","iopub.execute_input":"2025-08-05T23:28:10.368543Z","iopub.status.idle":"2025-08-05T23:28:11.543566Z","shell.execute_reply.started":"2025-08-05T23:28:10.36852Z","shell.execute_reply":"2025-08-05T23:28:11.542649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BigramNN(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # Just weights, no bias\n        self.W = nn.Parameter(torch.randn(vocab_size, vocab_size))  # 27 x 27\n\n    def forward(self, x_idx):\n        # x_idx: shape [batch], containing character indices\n        logits = self.W[x_idx]  # each row of W corresponds to logits for next char\n        return logits\n\nmodel = BigramNN(vocab_size)\nprint(f\"[INFO] Model initialized with weight shape: {model.W.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T18:56:02.824805Z","iopub.execute_input":"2025-08-05T18:56:02.8252Z","iopub.status.idle":"2025-08-05T18:56:02.837354Z","shell.execute_reply.started":"2025-08-05T18:56:02.825175Z","shell.execute_reply":"2025-08-05T18:56:02.836378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()  # applies log-softmax + NLLLoss\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.5)\n\n# Training step logging\ndef compute_loss():\n    logits = model(x_train)\n    loss = loss_fn(logits, y_train)\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T18:56:33.724834Z","iopub.execute_input":"2025-08-05T18:56:33.725215Z","iopub.status.idle":"2025-08-05T18:56:33.730505Z","shell.execute_reply.started":"2025-08-05T18:56:33.725196Z","shell.execute_reply":"2025-08-05T18:56:33.729555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop with early stopping\nfor epoch in range(100000):\n    loss = compute_loss()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(f\"[EPOCH {epoch}] Loss: {loss.item():.4f}\")\n    # 🛑 Early stop condition\n    if loss.item() < 1.0:\n        print(f\"[INFO] Early stopping at epoch {epoch} with loss {loss.item():.4f}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T18:56:37.22535Z","iopub.execute_input":"2025-08-05T18:56:37.225638Z","iopub.status.idle":"2025-08-05T20:00:00.378963Z","shell.execute_reply.started":"2025-08-05T18:56:37.225618Z","shell.execute_reply":"2025-08-05T20:00:00.377181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚠️ Why This Neural Network Still Isn't Efficient — And What Comes Next\n\nSo far, we’ve transitioned from a **count-based bigram model** to a **neural network** that uses:\n\n- A learned embedding for each character\n- A hidden layer with non-linearity\n- A softmax over the vocabulary\n\nThis model is more expressive than the raw count matrix, but **fundamentally limited in a critical way**:\n\n---\n\n## ❗ The Core Limitation: It's Still a Bigram Model\n\nEven though it uses neural components, our current model still only looks at **one character at a time** to predict the next:\n\n> **P(c₂ | c₁)** — based only on the previous single character.\n\nIt doesn’t matter whether we're using a count matrix or a neural net — the model **has no memory of what came before c₁**.\n\n---\n\n## 🤯 Why That’s a Problem\n\n- **Hallucination is common**: The model may generate \"realistic\" looking character transitions but quickly veer off into nonsense because there's no control or grammar.\n- **Misses long-term dependencies**: Words like `\"follow\"` require learning `'l'` after `'l'` only if it follows `'o'`, etc. Our model can't capture this.\n- **Fails on spelling structure**: It can’t distinguish `\"tio\"` in `\"nation\"` vs. `\"tion\"` in `\"action\"` — both just look like random character sequences.\n- **Low ceiling**: Even after extensive training, test loss rarely drops below `0.7`, because the model simply can’t “think ahead.”\n\n---\n\n## 🤖 Neural ≠ Intelligent (If Not Designed Right)\n\nJust because the model uses learnable layers and activations doesn’t mean it’s \"understanding\" sequences. This version is just:\n\n> A **smarter bigram table** — but still only a bigram table.\n\nIf the **input is only a single character**, the **best this network can do is what a count-based model does**, but with fancier weight tuning.\n\n---\n\n## 🧠 What We Need Instead: Sequence Models\n\nTo move beyond these limitations, we need to use models that can:\n\n✅ Take **multiple previous characters** into account  \n✅ Learn dependencies across **entire words or sequences**  \n✅ Build an **internal state** of context while generating  \n\nThis leads us to:\n\n### 🔄 Recurrent Neural Networks (RNNs)\n\n- Keep a **hidden state** across time steps\n- Can model **entire sequences** like `\"fol\"` → predict `\"l\"`\n\n### ⚡ Transformers\n\n- Use **self-attention** to see the whole context at once\n- Scale better and train faster than RNNs\n- Backbone of modern LLMs like GPT\n\n---\n\n## 🔁 Summary\n\n| Model               | Looks Back How Far? | Learns Sequence Context? | Efficient? |\n|--------------------|---------------------|---------------------------|------------|\n| Count-Based Bigram | 1 char              | ❌                        | ✅          |\n| Neural Bigram (ours)| 1 char             | ❌                        | ❌          |\n| RNN / Transformer   | Many chars          | ✅                        | ✅✅✅        |\n\n---\n\n## ⏭️ Next Step\n\n> We now need to move from **one-step character prediction** to **multi-step sequence modeling** — where the next character depends on the entire prefix.\n\nLet’s build a **recurrent model** or a **Transformer-style character-level language model** next — one that can *truly learn to spell*.\n","metadata":{}},{"cell_type":"code","source":"class BigramMLP(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x_idx):\n        x = self.embedding(x_idx)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\nmlp_model = BigramMLP(vocab_size)\nmlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.05)\nscheduler = torch.optim.lr_scheduler.StepLR(mlp_optimizer, step_size=200, gamma=0.5)\nmlp_loss_fn = nn.CrossEntropyLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T23:45:13.542906Z","iopub.execute_input":"2025-08-05T23:45:13.543159Z","iopub.status.idle":"2025-08-05T23:45:13.551668Z","shell.execute_reply.started":"2025-08-05T23:45:13.543142Z","shell.execute_reply":"2025-08-05T23:45:13.550446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss_mlp(x, y):\n    logits = mlp_model(x)\n    return mlp_loss_fn(logits, y)\n\nfor epoch in range(10000):\n    loss = compute_loss_mlp(x_train, y_train)\n    mlp_optimizer.zero_grad()\n    loss.backward()\n    mlp_optimizer.step()\n    scheduler.step()\n    \n    print(f\"[MLP] Epoch {epoch} - Train Loss: {loss.item():.4f}\")\n        \n    if loss.item() < 2.32:\n        print(f\"[MLP] Early stopping at epoch {epoch}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T23:45:20.649109Z","iopub.execute_input":"2025-08-05T23:45:20.64991Z","iopub.status.idle":"2025-08-05T23:58:22.416162Z","shell.execute_reply.started":"2025-08-05T23:45:20.649867Z","shell.execute_reply":"2025-08-05T23:58:22.41482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on test set\nwith torch.no_grad():\n    test_loss_mlp = compute_loss_mlp(x_test, y_test).item()\nprint(f\"[MLP] Final Test Loss: {test_loss_mlp:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T23:58:45.027841Z","iopub.execute_input":"2025-08-05T23:58:45.028122Z","iopub.status.idle":"2025-08-05T23:58:45.526622Z","shell.execute_reply.started":"2025-08-05T23:58:45.028102Z","shell.execute_reply":"2025-08-05T23:58:45.525415Z"}},"outputs":[],"execution_count":null}]}