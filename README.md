# ðŸ”¤ Character-Level Language Modeling: From Bigrams to Transformers

## ðŸ§  Project Goal

Our ultimate aim is to understand and build **Transformer-based models** for language modeling â€” the same foundation behind GPT and BERT. To truly appreciate their power, weâ€™ve taken a bottom-up journey, starting with **the simplest models** and gradually adding complexity and capability.

---

## ðŸš€ Journey So Far

We began with a **bigram-based character-level language model** using a raw count matrix. This model taught us the limitations of pure frequency-based approaches:

- No generalization to unseen character pairs
- Cannot learn structure or adapt
- Generates nonsensical or highly repetitive text

To overcome these flaws, we implemented:

### âœ… 1. **Neural Bigram Model**
- A single-layer neural net with a 27Ã—27 weight matrix
- Predicts the next character from a one-hot input
- **Limitations**: Still only looks one character back (no real sequence modeling)

### âœ… 2. **Feedforward Neural Network**
- Added hidden layers and non-linearities
- Improved generalization slightly
- **Still limited** by the lack of sequence awareness

### âœ… 3. **Recurrent Neural Network (RNN)**
- Introduced memory via hidden state
- Predicts next characters based on previous context
- Can learn spelling rules and character patterns
- Provides a solid base for understanding sequence modeling

---

## ðŸ“š Why This Progression?

Before jumping into Transformers, itâ€™s important to **understand what problems they solve**.

| Model        | Sequence Aware? | Learns from Context? | Handles Long-Term Dependencies? |
|--------------|------------------|-----------------------|----------------------------------|
| Bigram Count | âŒ              | âŒ                    | âŒ                               |
| Neural Net   | âŒ              | âœ… (slightly)         | âŒ                               |
| RNN          | âœ…              | âœ…                    | âŒ (limited memory)             |
| Transformer  | âœ…âœ…            | âœ…âœ…                  | âœ…âœ…âœ…                            |

> Each step in this project is designed to show **why Transformers are necessary**, not just how to use them.

---

## ðŸ” What's Next?

We plan to implement:

- âœ… **LSTM**: Better memory management for long-term dependencies
- ðŸš§ **Transformer Encoder**: Multi-head attention and self-attention for global context
- ðŸš€ **Autoregressive Transformer**: Character-level generative model (GPT-style)

---

## ðŸ§‘â€ðŸ’» Technologies Used

- Python + PyTorch
- Matplotlib for visualization
- Kaggle notebooks (for experimentation)
- English word corpus (character-level)

---

## ðŸ“ˆ Metrics Tracked

- Training loss & test loss
- Generated sample quality
- Learning curve improvements


---

## ðŸ™Œ Acknowledgments

Inspired by Andrej Karpathy's [NanoGPT](https://github.com/karpathy/nanogpt) and the fundamental ideas behind GPT models.

---

## ðŸ“Œ Final Thought

> To build strong models, you must understand weak ones.

This project is our foundational journey from basic statistics to cutting-edge sequence modeling â€” paving the way to truly grasp **how Transformers learn language**.
